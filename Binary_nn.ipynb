{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A binary classification Neural Network #\n",
    "\n",
    "Purpose of this notebook :\n",
    "\n",
    "- create a binary classification neural network from scratch\n",
    "- practice oop programming\n",
    "- practice docstring and commentaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add useful librairies\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :  (2, 900)\n",
      "Y :  (1, 900)\n",
      "X_test :  (2, 100)\n",
      "Y_test :  (1, 100)\n"
     ]
    }
   ],
   "source": [
    "# First the full neural network\n",
    "\n",
    "# The prupose is to predict from data\n",
    "# let's create some data a XOR problematic\n",
    "\n",
    "# Let's fix ou random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# A dataset of m example with 2 inputs shape(2 m)\n",
    "X = np.random.randn(2, 1000)\n",
    "# Transorm inputs in O. or 1.\n",
    "X = (X > 0.5) * 1.\n",
    "\n",
    "# The labels (when x = (0 0) or (1 1) > False, x = (0 1) or (1 0) > True)\n",
    "Y = (np.sum(X, axis=0, keepdims=True) == 1) * 1.\n",
    "\n",
    "# Add a test set\n",
    "train_size = int(X.shape[1] * 0.9)\n",
    "X_test = X[:,train_size:]\n",
    "Y_test = Y[:,train_size:]\n",
    "X = X[:,:train_size]\n",
    "Y = Y[:,:train_size]\n",
    "\n",
    "\n",
    "print('X : ', X.shape)\n",
    "print('Y : ', Y.shape)\n",
    "print('X_test : ', X_test.shape)\n",
    "print('Y_test : ', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 90)\n",
      "(1, 90)\n",
      "(2, 10)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "# Créer un dataset séparable par une ligne droite en 2D\n",
    "\n",
    "def split_dataset(X, y, train_pct=0.66):\n",
    "    \"\"\" Va séparer les datasets avec respect pour le pourcentage du dataset à mettre dans le train set.\n",
    "    Warning: La dimension des exemples doit être la première.\n",
    "    Warnings2: Cette fonction doit recevoir des exemples déjà mélangé (car splité en fonction des index)\n",
    "    \n",
    "    TODO: Rajouter un argument pour mélanger les dataset\n",
    "    \n",
    "    :X mes exemples, shape=(m, -1), m -> dimensions des exemples\n",
    "    :y mes labels, shape=(m, -1), m -> dimensions des exemples\n",
    "    :train_pct (default=0.66) Optionnal, c'est un pourcentage qui va séparer le de dataset avec train_pct * total_size dans le train set.\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test \n",
    "    \n",
    "\n",
    "    >>> X_train, y_train, X_test, y_test = split_dataset(X, y)\n",
    "    \"\"\"\n",
    "    # 2ème dimension -> celle des exemples\n",
    "    total_size = X.shape[1]\n",
    "    \n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape(1, -1)\n",
    "    \n",
    "    # On récupère `train_pct` % du dataset pour le train set, aussi il faut convertir en entier pour numpy ...\n",
    "    train_size = int(train_pct * total_size)\n",
    "    # ... et on met le reste danss le test set\n",
    "    test_size  = total_size - train_size \n",
    "    \n",
    "    # On met les `train_size` premier exemples/labels dans le train set ...\n",
    "    X_train, y_train = (X[:,:train_size], y[:,:train_size])\n",
    "    # ... et les test_size derniers exemples/labels dans le test set.\n",
    "    X_test , y_test  = (X[:,-test_size:], y[:,-test_size:])\n",
    "\n",
    "    # On s'assure que tous les exemples soient présents dans le test set ou le train set.\n",
    "    assert X_test.shape[1] + X_train.shape[1] == total_size\n",
    "    assert y_test.shape[1] + y_train.shape[1] == total_size\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X = np.zeros((2,100))\n",
    "Y = np.zeros((1,100))\n",
    "\n",
    "a, b, c, d = split_dataset(X,Y,0.9)\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(c.shape)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1800)\n",
      "(1, 1800)\n",
      "(2, 200)\n",
      "(1, 200)\n"
     ]
    }
   ],
   "source": [
    "# SET de MOON\n",
    "# On fixe le nombre d'example total par dataset à SAMPLE_SIZE\n",
    "SAMPLE_SIZE = 2000\n",
    "\n",
    "# Fixer le hasard\n",
    "np.random.seed(666)\n",
    "\n",
    "# On crée le dataset séparable linéarement\n",
    "X, Y = make_moons(n_samples = SAMPLE_SIZE,\n",
    "                          shuffle = True,\n",
    "                          noise = 0.1)\n",
    "\n",
    "X = X.T\n",
    "Y = Y.T\n",
    "\n",
    "# On split le dataset en set de training et de test\n",
    "X, Y, X_test, Y_test = split_dataset(X, Y, 0.9)\n",
    "\n",
    "# On affiche le shape pour la forme\n",
    "for each in (X, Y, X_test, Y_test):\n",
    "    print(each.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 660)\n",
      "(1, 660)\n",
      "(2, 340)\n",
      "(1, 340)\n",
      "[[ 0.92787748 -0.54303182  0.9246533  ...  0.75209229 -0.69471802\n",
      "  -0.08402948]\n",
      " [-0.04521731 -0.75444674 -0.71492522 ... -0.73132156 -0.90889854\n",
      "  -0.96805179]]\n"
     ]
    }
   ],
   "source": [
    "# CIRCLE SET\n",
    "# On fixe le nombre d'example total par dataset à SAMPLE_SIZE\n",
    "SAMPLE_SIZE = 1000\n",
    "\n",
    "# Fixer le hasard\n",
    "np.random.seed(1)\n",
    "\n",
    "# On crée le dataset séparable linéarement\n",
    "X, y = make_circles(n_samples = SAMPLE_SIZE,\n",
    "                          shuffle = True,\n",
    "                          noise = 0.1)\n",
    "X = X.T\n",
    "y = y.T\n",
    "\n",
    "# On split le dataset en set de training et de test\n",
    "X, Y, X_test, Y_test = split_dataset(X, y)\n",
    "\n",
    "# On affiche le shape pour la forme\n",
    "for each in (X, Y, X_test, Y_test):\n",
    "    print(each.shape)\n",
    "    \n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coding the sigmoid\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# testons la sigmoid\n",
    "sigmoid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.90243391 -0.56847539  0.89920974 ...  0.72664872 -0.72016158\n",
      "  -0.10947305]\n",
      " [-0.07066087 -0.77989031 -0.74036878 ... -0.75676513 -0.93434211\n",
      "  -0.99349536]]\n",
      "[[ 0.74011604 -0.46622556  0.73747179 ...  0.59594876 -0.59062845\n",
      "  -0.08978248]\n",
      " [-0.05795133 -0.63961396 -0.60720104 ... -0.62064823 -0.76628501\n",
      "  -0.81479856]]\n"
     ]
    }
   ],
   "source": [
    "# Normalisation\n",
    "X = X - (np.amax(X) + np.amin(X)) / 2\n",
    "print(X)\n",
    "X = X / np.amax(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.694267591910045\n",
      "0.6823533921157102\n",
      "0.6823527286774689\n",
      "0.6823527286774514\n",
      "0.6823527286774512\n",
      "0.6823527286774511\n",
      "0.6823527286774511\n",
      "0.6823527286774511\n",
      "0.6823527286774511\n",
      "0.6823527286774511\n",
      "0.6823527286774512\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774512\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774512\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774512\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774511\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774511\n",
      "0.6823527286774512\n",
      "0.6823527286774511\n",
      "0.6823527286774511\n",
      "0.6823527286774511\n",
      "0.6823527286774511\n",
      "0.6823527286774511\n",
      "0.6823527286774511\n",
      "0.6823527286774512\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774512\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774512\n",
      "0.6823527286774511\n",
      "0.6823527286774511\n",
      "0.6823527286774511\n",
      "0.6823527286774511\n",
      "0.6823527286774511\n",
      "0.6823527286774511\n",
      "0.6823527286774512\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774512\n",
      "0.6823527286774511\n",
      "0.6823527286774511\n",
      "0.6823527286774511\n",
      "0.6823527286774511\n",
      "0.6823527286774511\n",
      "0.6823527286774511\n",
      "0.6823527286774511\n",
      "0.6823527286774512\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n",
      "0.6823527286774514\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train set :  57.33333333333333\n",
      "Loss test set :  0.6823527286774511\n",
      "Accuracy test set :  54.0\n"
     ]
    }
   ],
   "source": [
    "# let's concatenate all our cells\n",
    "\n",
    "# parameters\n",
    "np.random.seed(42)\n",
    "hidden_layers = [4, 3, 2]\n",
    "iteration = 10000\n",
    "learning_rate = 0.1\n",
    "\n",
    "layers = [X.shape[0]] + hidden_layers + [1]\n",
    "# we need a list to store our costs\n",
    "costs = []\n",
    "\n",
    "# Initialisation of list Weights and bias shape (output_layer(-1) layer) (output_layer(-1) 1)\n",
    "W = [np.random.randn(layers[k], layers[k-1]) for k in range(1, len(layers))]\n",
    "b = [np.zeros((layers[k], 1)) for k in range(1, len(layers))]\n",
    "\n",
    "# training loop\n",
    "for k in range(iteration):\n",
    "    # Forward -------------------------------------------------\n",
    "    # Let's make a list of activation starting with the inputs matrix\n",
    "    activation = [X]\n",
    "    # Loop to pass through all neurons\n",
    "    for l in range(len(layers)-1):\n",
    "    #     print(activation[l],' * ',W[l],' + ',b)\n",
    "        pre_activation = np.dot(W[l], activation[l]) + b[l]\n",
    "    #     print('preactivation :',pre_activation)\n",
    "        activation.append(sigmoid(pre_activation))\n",
    "\n",
    "    # loss and cost ----------------------------------\n",
    "    prediction_inter = activation[-1]\n",
    "    # loss function\n",
    "    loss = -Y * np.log(prediction_inter) - (1 - Y) * np.log(1 - prediction_inter)\n",
    "    # number of examples\n",
    "    m = X.shape[1]\n",
    "    # Cost function\n",
    "    cost = np.sum(loss) / m\n",
    "    if k % 100 == 0:\n",
    "        print(cost)\n",
    "    # Cost storage\n",
    "    costs.append(cost)\n",
    "\n",
    "    # Backward --------------------------------------------------\n",
    "\n",
    "    # First the last layer\n",
    "    d_loss = -Y/activation[-1] + (1-Y)/(1-activation[-1])\n",
    "    d_activation = activation[-1] * (1 - activation[-1])\n",
    "    d_pre_activation = d_loss * d_activation\n",
    "#     d_pre_activation = (activation[-1] - Y)\n",
    "\n",
    "    d_W = [np.dot(d_pre_activation, activation[-2].T)]\n",
    "    d_b = [np.sum(d_pre_activation, axis=1, keepdims=True) / m]\n",
    "\n",
    "    # Loop for the next layers\n",
    "    for l in reversed(range(1, len(layers)-1)):\n",
    "        d_pre_activation = np.dot(W[l].T, d_pre_activation)\n",
    "        d_pre_activation_sig = d_pre_activation * (activation[l] * (1 - activation[l]))\n",
    "\n",
    "        d_W.insert(0, np.dot(d_pre_activation_sig, activation[l-1].T))\n",
    "        d_b.insert(0, np.sum(d_pre_activation_sig, axis=1, keepdims=True) / m)\n",
    "\n",
    "    # updating the parameters\n",
    "    # Loop for updating all weights and bias in the network\n",
    "    for i in range(len(layers)-1):\n",
    "        W[i] -= learning_rate * d_W[i]\n",
    "        b[i] -= learning_rate * d_b[i]\n",
    "\n",
    "# Let's show the loss evolution\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "plt.show()\n",
    "\n",
    "# Let's compute the accuracy\n",
    "prediction = (activation[-1] > 0.5) * 1\n",
    "accuracy = (1 - np.mean(np.abs(Y - prediction))) * 100\n",
    "print('Accuracy train set : ', accuracy)\n",
    "        \n",
    "# Now the test set to compare\n",
    "# Forward -------------------------------------------------\n",
    "# Let's make a list of activation starting with the inputs matrix\n",
    "activation_test = [X_test]\n",
    "# Loop to pass through all neurons\n",
    "for l in range(len(layers)-1):\n",
    "    pre_activation = np.dot(W[l], activation_test[l]) + b[l]\n",
    "    activation_test.append(sigmoid(pre_activation))\n",
    "\n",
    "# loss and cost ----------------------------------\n",
    "prediction_test_inter = activation_test[-1]\n",
    "# loss function\n",
    "loss_test = -Y_test * np.log(prediction_test_inter) - (1 - Y_test) * np.log(1 - prediction_test_inter)\n",
    "# number of examples\n",
    "m_test = X_test.shape[1]\n",
    "# Cost function\n",
    "cost_test = np.sum(loss) / m_test\n",
    "print('Loss test set : ', cost)\n",
    "    \n",
    "# Let's compute the accuracy\n",
    "prediction_test = (activation_test[-1] > 0.5) * 1\n",
    "accuracy_test = (1 - np.mean(np.abs(Y_test - prediction_test))) * 100\n",
    "print('Accuracy test set : ', accuracy_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
