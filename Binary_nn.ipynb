{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A binary classification Neural Network #\n",
    "\n",
    "Purpose of this notebook :\n",
    "\n",
    "- create a binary classification neural network from scratch\n",
    "- practice oop programming\n",
    "- practice docstring and commentaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add useful librairies\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : \n",
      " [[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "Y : \n",
      " [[0. 0. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# First the full neural network\n",
    "\n",
    "# The prupose is to predict from data\n",
    "# let's create some data a XOR problematic\n",
    "\n",
    "# Let's fix ou random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# A dataset of m example with 2 inputs shape(2 m)\n",
    "X = np.random.randn(2, 100)\n",
    "# Transorm inputs in O. or 1.\n",
    "X = (X > 0.5) * 1.\n",
    "\n",
    "# The labels (when x = (0 0) or (1 1) > False, x = (0 1) or (1 0) > True)\n",
    "Y = (np.sum(X, axis=0, keepdims=True) == 1) * 1.\n",
    "\n",
    "print('X : \\n', X[:,:5])\n",
    "print('Y : \\n', Y[:,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coding the sigmoid\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# testons la sigmoid\n",
    "sigmoid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6759343996579917\n",
      "0.19232834621526615\n",
      "0.1917416981014084\n",
      "0.1913640541323848\n",
      "0.191238571863169\n",
      "0.19118329393667807\n",
      "0.19115133556055056\n",
      "0.19113051854714003\n",
      "0.1911159050893599\n",
      "0.19110509690299246\n"
     ]
    }
   ],
   "source": [
    "# let's concatenate all our cells\n",
    "\n",
    "# parameters\n",
    "np.random.seed(42)\n",
    "hidden_layers = [7,2]\n",
    "iteration = 1000\n",
    "learning_rate = 0.4\n",
    "layers = [X.shape[0]] + hidden_layers + [1]\n",
    "\n",
    "# Initialisation of list Weights and bias shape (output_layer(-1) layer) (output_layer(-1) 1)\n",
    "W = [np.random.randn(layers[k], layers[k-1]) for k in range(1, len(layers))]\n",
    "b = [np.zeros((layers[k], 1)) for k in range(1, len(layers))]\n",
    "\n",
    "# training loop\n",
    "for k in range(iteration):\n",
    "    # Forward -------------------------------------------------\n",
    "    # Let's make a list of activation starting with the inputs matrix\n",
    "    activation = [X]\n",
    "    # Loop to pass through all neurons\n",
    "    for l in range(len(layers)-1):\n",
    "    #     print(activation[l],' * ',W[l],' + ',b)\n",
    "        pre_activation = np.dot(W[l], activation[l]) + b[l]\n",
    "    #     print('preactivation :',pre_activation)\n",
    "        activation.append(sigmoid(pre_activation))\n",
    "\n",
    "    # loss and cost ----------------------------------\n",
    "    prediction_inter = activation[-1]\n",
    "    # loss function\n",
    "    loss = -Y * np.log(prediction_inter) - (1 - Y) * np.log(1 - prediction_inter)\n",
    "    # number of examples\n",
    "    m = X.shape[1]\n",
    "    # Cost function\n",
    "    cost = np.sum(loss) / m\n",
    "    if k % 100 == 0:\n",
    "        print(cost)\n",
    "\n",
    "    # Backward --------------------------------------------------\n",
    "\n",
    "    # First the last layer\n",
    "    d_loss = -Y/activation[-1] + (1-Y)/(1-activation[-1])\n",
    "    d_activation = activation[-1] * (1 - activation[-1])\n",
    "    d_pre_activation = d_loss * d_activation\n",
    "\n",
    "    d_W = [np.dot(d_pre_activation, activation[-2].T)]\n",
    "    d_b = [np.sum(d_pre_activation, axis=1, keepdims=True) / m]\n",
    "\n",
    "    # Loop for the next layers\n",
    "    for l in reversed(range(1, len(layers)-1)):\n",
    "        d_pre_activation = np.dot(W[l].T, d_pre_activation)\n",
    "        d_pre_activation_sig = d_pre_activation * activation[l] * (1 - activation[l])\n",
    "\n",
    "        d_W.insert(0, np.dot(d_pre_activation_sig, activation[l-1].T))\n",
    "        d_b.insert(0, np.sum(d_pre_activation_sig, axis=1, keepdims=True) / m)\n",
    "\n",
    "    # updating the parameters\n",
    "    # Loop for updating all weights and bias in the network\n",
    "    for i in range(len(layers)-1):\n",
    "        W[i] -= learning_rate * d_W[i]\n",
    "        b[i] -= learning_rate * d_b[i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
