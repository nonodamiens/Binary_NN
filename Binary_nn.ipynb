{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A binary classification Neural Network #\n",
    "\n",
    "Purpose of this notebook :\n",
    "\n",
    "- create a binary classification neural network from scratch\n",
    "- practice oop programming\n",
    "- practice docstring and commentaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add useful librairies\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : \n",
      " [[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "Y : \n",
      " [[0. 0. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# First the full neural network\n",
    "\n",
    "# The prupose is to predict from data\n",
    "# let's create some data a XOR problematic\n",
    "\n",
    "# Let's fix ou random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# A dataset of m example with 2 inputs shape(2 m)\n",
    "X = np.random.randn(2, 100)\n",
    "# Transorm inputs in O. or 1.\n",
    "X = (X > 0.5) * 1.\n",
    "\n",
    "# The labels (when x = (0 0) or (1 1) > False, x = (0 1) or (1 0) > True)\n",
    "Y = (np.sum(X, axis=0, keepdims=True) == 1) * 1.\n",
    "\n",
    "print('X : \\n', X[:,:5])\n",
    "print('Y : \\n', Y[:,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1]\n",
      " [1 1 0 0]]\n",
      "[[1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,1,0,1],[1,1,0,0]])\n",
    "Y = np.array([[1,0,0,1]])\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers :  [2, 3, 1]\n",
      "shape W 0  : (3, 2)\n",
      "shape W 1  : (1, 3)\n",
      "shape b 0  :  (3, 1)\n",
      "shape b 1  :  (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Initialisation\n",
    "# List of hidden layers\n",
    "hidden_layers = [3]\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# List of layers for binary classification\n",
    "layers = [X.shape[0]] + hidden_layers + [1]\n",
    "\n",
    "# Initialisation of list Weights and bias shape (output_layer(-1) layer) (output_layer(-1) 1)\n",
    "W = [np.random.randn(layers[k], layers[k-1]) for k in range(1, len(layers))]\n",
    "b = [np.zeros((layers[k], 1)) for k in range(1, len(layers))]\n",
    "\n",
    "print('layers : ', layers)\n",
    "for j,i in enumerate(W):\n",
    "    print('shape W',j,' :',i.shape)\n",
    "for j, i in enumerate(b):\n",
    "    print('shape b',j,' : ', i.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coding the sigmoid\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# testons la sigmoid\n",
    "sigmoid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4)\n",
      "(3, 4)\n",
      "(1, 4)\n",
      "[[0.8 0.8 0.7 0.8]]\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "\n",
    "# Let's make a list of activation starting with the inputs matrix\n",
    "activation = [X]\n",
    "\n",
    "# Loop to pass through all neurons\n",
    "for l in range(len(layers)-1):\n",
    "#     print(activation[l],' * ',W[l],' + ',b)\n",
    "    pre_activation = np.dot(W[l], activation[l]) + b[l]\n",
    "#     print('preactivation :',pre_activation)\n",
    "    activation.append(np.around(sigmoid(pre_activation),1))\n",
    "\n",
    "# check the activations shape    \n",
    "    print(activation[l].shape)\n",
    "print(activation[-1].shape)\n",
    "\n",
    "# Check activation values\n",
    "print(activation[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.22314355 1.60943791 1.2039728  0.22314355]]\n",
      "0.814924454847114\n"
     ]
    }
   ],
   "source": [
    "# What about the loss and cost\n",
    "\n",
    "prediction_inter = activation[-1]\n",
    "\n",
    "# loss function\n",
    "loss = -Y * np.log(prediction_inter) - (1 - Y) * np.log(1 - prediction_inter)\n",
    "print(loss)\n",
    "\n",
    "# number of examples\n",
    "m = X.shape[1]\n",
    "# Cost function\n",
    "cost = np.sum(loss) / m\n",
    "\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw : (3, 2)\n",
      "dw : (1, 3)\n",
      "db : (3, 1)\n",
      "db : (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Backward\n",
    "\n",
    "# First the last layer\n",
    "d_loss = -Y/activation[-1] + (1-Y)/(1-activation[-1])\n",
    "d_activation = activation[-1] * (1 - activation[-1])\n",
    "d_pre_activation = d_loss * d_activation\n",
    "\n",
    "d_W = [np.dot(d_pre_activation, activation[-2].T)]\n",
    "d_b = [np.sum(d_pre_activation, axis=1, keepdims=True) / m]\n",
    "\n",
    "# Loop for the next layers\n",
    "for l in reversed(range(1, len(layers)-1)):\n",
    "    d_pre_activation = np.dot(W[l].T, d_pre_activation)\n",
    "    d_pre_activation_sig = d_pre_activation * activation[l] * (1 - activation[l])\n",
    "    \n",
    "    d_W.insert(0, np.dot(d_pre_activation_sig, activation[l-1].T))\n",
    "    d_b.insert(0, np.sum(d_pre_activation_sig, axis=1, keepdims=True) / m)\n",
    "\n",
    "# We can check the shapes\n",
    "for i in d_W:\n",
    "    print('dw :', i.shape)\n",
    "for i in d_b:\n",
    "    print('db :', i.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new W 0  :  (3, 2)\n",
      "new b 0  :  (3, 1)\n",
      "new W 1  :  (1, 3)\n",
      "new b 1  :  (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# updating the parameters\n",
    "\n",
    "# Setting the learning rate\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Loop for updating all weights and bias in the network\n",
    "for i in range(len(layers)-1):\n",
    "    W[i] -= learning_rate * d_W[i]\n",
    "    b[i] -= learning_rate * d_b[i]\n",
    "    \n",
    "# Checking shapes\n",
    "    print('new W', i, ' : ', W[i].shape)\n",
    "    print('new b', i, ' : ', b[i].shape)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
